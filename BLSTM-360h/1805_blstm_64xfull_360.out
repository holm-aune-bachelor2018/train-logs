/home/marith1/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
WARNING:tensorflow:From /home/marith1/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
2018-05-18 12:27:47.802127: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-18 12:27:49.330865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:03:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-18 12:27:49.500521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:82:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-18 12:27:49.500590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0, 1
2018-05-18 12:27:54.382278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-18 12:27:54.382339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 
2018-05-18 12:27:54.382348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N 
2018-05-18 12:27:54.382353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N 
2018-05-18 12:27:54.383051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15133 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 6.0)
2018-05-18 12:27:54.536052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15133 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0, compute capability: 6.0)

Reading training data:
('Reading csv:', '/home/marith1/projects/ctc/data_dir/librivox-train-clean-360.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 104014)
('Total number of files (after reduction):', 102100)
('max_intseq_length:', 280)
('numclasses:', 29)
('max_trans_charlength:', 280)
('Words:', 3490549)
('Vocab:', 59235)

Reading validation data: 
('Reading csv:', '/home/marith1/projects/ctc/data_dir/librivox-dev-clean.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 2703)
('Total number of files (after reduction):', 2616)
('max_intseq_length:', 279)
('numclasses:', 29)
('max_trans_charlength:', 279)
('Words:', 48972)
('Vocab:', 7726)

Reading test data: 
('Reading csv:', '/home/marith1/projects/ctc/data_dir/librivox-test-clean.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 2620)
('Total number of files (after reduction):', 2526)
('max_intseq_length:', 280)
('numclasses:', 29)
('max_trans_charlength:', 280)
('Words:', 46839)
('Vocab:', 7547)


Model and training parameters: 
Starting time:  2018-05-18 12:27:39
 - epochs:  60 
 - batch size:  64 
 - input epoch length:  0 
 - network epoch length:  1595 
 - training on  102080  files 
 - learning rate:  0.0001 
 - hidden units:  512 
 - mfcc features:  26 
 - dropout:  0.2 

Creating new model:  dnn_blstm
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
the_input (InputLayer)          (None, None, 26)     0                                            
__________________________________________________________________________________________________
fc_1 (TimeDistributed)          (None, None, 512)    13824       the_input[0][0]                  
__________________________________________________________________________________________________
dropout_1 (TimeDistributed)     (None, None, 512)    0           fc_1[0][0]                       
__________________________________________________________________________________________________
fc_2 (TimeDistributed)          (None, None, 512)    262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (TimeDistributed)     (None, None, 512)    0           fc_2[0][0]                       
__________________________________________________________________________________________________
fc_3 (TimeDistributed)          (None, None, 512)    262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dropout_3 (TimeDistributed)     (None, None, 512)    0           fc_3[0][0]                       
__________________________________________________________________________________________________
CuDNN_bi_lstm (Bidirectional)   (None, None, 512)    4202496     dropout_3[0][0]                  
__________________________________________________________________________________________________
fc_4 (TimeDistributed)          (None, None, 512)    262656      CuDNN_bi_lstm[0][0]              
__________________________________________________________________________________________________
dropout_4 (TimeDistributed)     (None, None, 512)    0           fc_4[0][0]                       
__________________________________________________________________________________________________
softmax (TimeDistributed)       (None, None, 29)     14877       dropout_4[0][0]                  
__________________________________________________________________________________________________
the_labels (InputLayer)         (None, None)         0                                            
__________________________________________________________________________________________________
input_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
label_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    
                                                                 the_labels[0][0]                 
                                                                 input_length[0][0]               
                                                                 label_length[0][0]               
==================================================================================================
Total params: 5,019,165
Trainable params: 5,019,165
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/60
 - 3556s - loss: 370.5421 - val_loss: 136.3423

Epoch 00001: val_loss improved from inf to 136.34234, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.895703125 

Epoch 2/60
 - 2783s - loss: 233.3652 - val_loss: 107.8220

Epoch 00002: val_loss improved from 136.34234 to 107.82197, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.838485863095 

Epoch 3/60
 - 2706s - loss: 194.3529 - val_loss: 93.0383

Epoch 00003: val_loss improved from 107.82197 to 93.03827, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.784933035714 

Epoch 4/60
 - 2741s - loss: 171.4181 - val_loss: 84.6224

Epoch 00004: val_loss improved from 93.03827 to 84.62242, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.739713541667 

Epoch 5/60
 - 2762s - loss: 155.6686 - val_loss: 77.9060

Epoch 00005: val_loss improved from 84.62242 to 77.90603, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.747005208333 

Epoch 6/60
 - 2791s - loss: 143.8153 - val_loss: 72.6615

Epoch 00006: val_loss improved from 77.90603 to 72.66150, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.734895833333 

Epoch 7/60
 - 2831s - loss: 134.4151 - val_loss: 68.9391

Epoch 00007: val_loss improved from 72.66150 to 68.93908, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.726023065476 

Epoch 8/60
 - 2716s - loss: 126.7700 - val_loss: 65.5680

Epoch 00008: val_loss improved from 68.93908 to 65.56799, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.717912946429 

Epoch 9/60
 - 2692s - loss: 120.3154 - val_loss: 62.7098

Epoch 00009: val_loss improved from 65.56799 to 62.70984, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.697470238095 

Epoch 10/60
 - 2722s - loss: 114.6753 - val_loss: 60.4010

Epoch 00010: val_loss improved from 62.70984 to 60.40099, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.673865327381 

Log file saved:  results/1805_blstm_64xfull_360_05-18_1227.csv
Epoch 11/60
 - 2668s - loss: 109.9078 - val_loss: 58.8557

Epoch 00011: val_loss improved from 60.40099 to 58.85568, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.682533482143 

Epoch 12/60
 - 2686s - loss: 105.6232 - val_loss: 56.3389

Epoch 00012: val_loss improved from 58.85568 to 56.33889, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.632998511905 

Epoch 13/60
 - 2783s - loss: 101.8086 - val_loss: 54.7444

Epoch 00013: val_loss improved from 56.33889 to 54.74438, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.623493303571 

Epoch 14/60
 - 2683s - loss: 98.4012 - val_loss: 53.3388

Epoch 00014: val_loss improved from 54.74438 to 53.33879, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.612258184524 

Epoch 15/60
 - 2770s - loss: 95.3225 - val_loss: 52.4544

Epoch 00015: val_loss improved from 53.33879 to 52.45438, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.620219494048 

Epoch 16/60
 - 2758s - loss: 92.5091 - val_loss: 51.0586

Epoch 00016: val_loss improved from 52.45438 to 51.05860, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.616592261905 

Epoch 17/60
 - 2730s - loss: 89.9696 - val_loss: 49.8777

Epoch 00017: val_loss improved from 51.05860 to 49.87774, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.590271577381 

Epoch 18/60
 - 2757s - loss: 87.5748 - val_loss: 48.4417

Epoch 00018: val_loss improved from 49.87774 to 48.44168, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.568638392857 

Epoch 19/60
 - 2741s - loss: 85.3336 - val_loss: 48.2181

Epoch 00019: val_loss improved from 48.44168 to 48.21813, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.588188244048 

Epoch 20/60
 - 2661s - loss: 83.3531 - val_loss: 47.1584

Epoch 00020: val_loss improved from 48.21813 to 47.15840, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.59296875 

Log file saved:  results/1805_blstm_64xfull_360_05-18_1227.csv
Epoch 21/60
 - 2668s - loss: 81.5508 - val_loss: 46.8309

Epoch 00021: val_loss improved from 47.15840 to 46.83091, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.550911458333 

Epoch 22/60
 - 2755s - loss: 79.7239 - val_loss: 46.3680

Epoch 00022: val_loss improved from 46.83091 to 46.36805, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.561774553571 

Epoch 23/60
 - 2767s - loss: 78.1328 - val_loss: 45.2521

Epoch 00023: val_loss improved from 46.36805 to 45.25210, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.550837053571 

Epoch 24/60
 - 2775s - loss: 76.5989 - val_loss: 44.7577

Epoch 00024: val_loss improved from 45.25210 to 44.75770, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.544215029762 

Epoch 25/60
 - 2708s - loss: 75.1030 - val_loss: 43.8741

Epoch 00025: val_loss improved from 44.75770 to 43.87413, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.527938988095 

Epoch 26/60
 - 2797s - loss: 73.7678 - val_loss: 43.7943

Epoch 00026: val_loss improved from 43.87413 to 43.79429, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.558407738095 

Epoch 27/60
 - 2813s - loss: 72.4832 - val_loss: 42.7591

Epoch 00027: val_loss improved from 43.79429 to 42.75907, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.520014880952 

Epoch 28/60
 - 2761s - loss: 71.1962 - val_loss: 42.3005

Epoch 00028: val_loss improved from 42.75907 to 42.30051, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.498270089286 

Epoch 29/60
 - 2772s - loss: 70.1282 - val_loss: 41.9065

Epoch 00029: val_loss improved from 42.30051 to 41.90651, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.490662202381 

Epoch 30/60
 - 2810s - loss: 68.9950 - val_loss: 41.3200

Epoch 00030: val_loss improved from 41.90651 to 41.31998, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.505357142857 

Log file saved:  results/1805_blstm_64xfull_360_05-18_1227.csv
Epoch 31/60
 - 2742s - loss: 67.9419 - val_loss: 41.3760

Epoch 00031: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.512555803571 

Epoch 32/60
 - 2779s - loss: 66.8977 - val_loss: 40.7965

Epoch 00032: val_loss improved from 41.31998 to 40.79655, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.50818452381 

Epoch 33/60
 - 2812s - loss: 66.0078 - val_loss: 40.3401

Epoch 00033: val_loss improved from 40.79655 to 40.34013, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.486644345238 

Epoch 34/60
 - 2665s - loss: 65.0663 - val_loss: 40.5835

Epoch 00034: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.499795386905 

Epoch 35/60
 - 2815s - loss: 64.1971 - val_loss: 40.4097

Epoch 00035: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.491722470238 

Epoch 36/60
 - 2852s - loss: 63.3648 - val_loss: 40.2059

Epoch 00036: val_loss improved from 40.34013 to 40.20589, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.507552083333 

Epoch 37/60
 - 2749s - loss: 62.5282 - val_loss: 39.5941

Epoch 00037: val_loss improved from 40.20589 to 39.59411, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.484095982143 

Epoch 38/60
 - 2850s - loss: 61.7460 - val_loss: 39.3368

Epoch 00038: val_loss improved from 39.59411 to 39.33679, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.469773065476 

Epoch 39/60
 - 2823s - loss: 61.0142 - val_loss: 39.1154

Epoch 00039: val_loss improved from 39.33679 to 39.11541, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.487946428571 

Epoch 40/60
 - 2885s - loss: 60.3014 - val_loss: 38.4347

Epoch 00040: val_loss improved from 39.11541 to 38.43470, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.486049107143 

Log file saved:  results/1805_blstm_64xfull_360_05-18_1227.csv
Epoch 41/60
 - 2823s - loss: 59.5942 - val_loss: 38.8461

Epoch 00041: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.511830357143 

Epoch 42/60
 - 2670s - loss: 58.9284 - val_loss: 38.5316

Epoch 00042: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.467652529762 

Epoch 43/60
 - 2697s - loss: 58.2574 - val_loss: 38.1546

Epoch 00043: val_loss improved from 38.43470 to 38.15463, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.44763764881 

Epoch 44/60
 - 2773s - loss: 57.6459 - val_loss: 38.2178

Epoch 00044: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.489713541667 

Epoch 45/60
 - 2666s - loss: 56.9879 - val_loss: 37.5948

Epoch 00045: val_loss improved from 38.15463 to 37.59479, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.416964285714 

Epoch 46/60
 - 2739s - loss: 56.4498 - val_loss: 37.2408

Epoch 00046: val_loss improved from 37.59479 to 37.24078, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.435453869048 

Epoch 47/60
 - 2786s - loss: 55.9014 - val_loss: 37.8190

Epoch 00047: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.441108630952 

Epoch 48/60
 - 2747s - loss: 55.2608 - val_loss: 37.7428

Epoch 00048: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.473474702381 

Epoch 49/60
 - 2747s - loss: 54.7538 - val_loss: 36.9149

Epoch 00049: val_loss improved from 37.24078 to 36.91487, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.479055059524 

Epoch 50/60
 - 2667s - loss: 54.2497 - val_loss: 37.2632

Epoch 00050: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.467392113095 

Log file saved:  results/1805_blstm_64xfull_360_05-18_1227.csv
Epoch 51/60
 - 2692s - loss: 53.7237 - val_loss: 37.1379

Epoch 00051: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.442485119048 

Epoch 52/60
 - 2786s - loss: 53.3074 - val_loss: 36.8657

Epoch 00052: val_loss improved from 36.91487 to 36.86569, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.452008928571 

Epoch 53/60
 - 2677s - loss: 52.7999 - val_loss: 37.4586

Epoch 00053: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.474144345238 

Epoch 54/60
 - 2803s - loss: 52.3114 - val_loss: 36.7239

Epoch 00054: val_loss improved from 36.86569 to 36.72390, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.456436011905 

Epoch 55/60
 - 2807s - loss: 51.8781 - val_loss: 37.0648

Epoch 00055: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.458426339286 

Epoch 56/60
 - 2796s - loss: 51.4356 - val_loss: 36.5538

Epoch 00056: val_loss improved from 36.72390 to 36.55382, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.46095610119 

Epoch 57/60
 - 2809s - loss: 50.9816 - val_loss: 36.4295

Epoch 00057: val_loss improved from 36.55382 to 36.42947, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.437555803571 

Epoch 58/60
 - 2793s - loss: 50.5747 - val_loss: 36.5100

Epoch 00058: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.446912202381 

Epoch 59/60
 - 2802s - loss: 50.1508 - val_loss: 36.7207

Epoch 00059: val_loss did not improve
calc wer for 40  epoch lenght
 - average WER:  0.419568452381 

Epoch 60/60
 - 2691s - loss: 49.7816 - val_loss: 36.3835

Epoch 00060: val_loss improved from 36.42947 to 36.38346, saving model to models/1805_blstm_64xfull_360_best
calc wer for 40  epoch lenght
 - average WER:  0.460714285714 

Log file saved:  results/1805_blstm_64xfull_360_05-18_1227.csv
calc wer for 39  epoch lenght

 - Training ended, test wer:  0.474962797619  -

Prediction samples:

Original:  to make good home made bread                                    
Predicted:  to make good home mad breand 

Original:  i made a quick glance over my shoulder and grabbed at my gun    
Predicted:  a mad a quick glance o ri h older and grad t my dgone 

Original:  at all events we shall get there some day                       
Predicted:  at all evens we shall get there some d 

Original:  watch the savages outside said robert                           
Predicted:  watch ta saagis out side aid rover 

Original:  he seemed to be cursing people who had wronged him              
Predicted:  he seemed to be cursing people would wrounged him 

Original:  i'll be glad to try sir he replied                              
Predicted:  i'll be glad to trisir ye replied 

Original:  impertinent young beggar said burgess                           
Predicted:  impertnete young peggar said burges 

Original:  i had the pleasure of meeting him in society                    
Predicted:  i had the pleasuref meding him in society 

Original:  rosecrans protested it was in vain                              
Predicted:  ros crans protested it was invain 

Original:  he was stone dead but i dropped that foot quick                 
Predicted:  he was stone deadd but i droped at foot quick 

Original:  couldn't help it then how can i ever trust you                  
Predicted:  couold in't help it than how can i eveer trustio 

Original:  he also thought of his managerial position                      
Predicted:  the also thought of his manotgerial position 

Original:  then bozzle came forward and introduced his wife                
Predicted:  then thoso kame forward an introduced his wife 

Original:  i do not know confessed shaggy                                  
Predicted:  i do nat know confess chage 

Original:  in about an hour and three quarters the boy returned            
Predicted:  in about an or and three qurters the boy recarned 

Original:  illustration rusks                                              
Predicted:  ilustration rusks 

Original:  illustration ginger                                             
Predicted:  ilistration ginger 

Original:  i was quartered with him at sarah island                        
Predicted:  i was quarterd with him at ser island 

Original:  i'm going to do what you asked me to do when you were in london 
Predicted:  a'md going tod do it you asked  eu dyo do when you were an london 

Original:  boats put out both from the fort and the shore                  
Predicted:  bots put ut both from the ford an the shore 

Original:  he could hardly realise how it had all come about               
Predicted:  he cound hardly realize how it it all come about 

Original:  must stop that fifty lashes troke                               
Predicted:  must stope that fifty lashes troke 

Original:  yes and a very respectable one                                  
Predicted:  es and a very respectable one 

Original:  but clews there are absolutely none                             
Predicted:  but closed there are absolutly non 

Original:  he pulled up a window as if the air were heavy                  
Predicted:  he pued up a window as if the air were heavy 

Original:  sweetwater help me out of this                                  
Predicted:  sweeweter helth me ot of thes 

Original:  with that came a pang of intense pain                           
Predicted:  wet bat came o pang of intense tpan 

Original:  for ambrosch my mama come here                                  
Predicted:  for ambrosh mi am an te come here 

Original:  i will sit in the parlor awhile and collect my thoughts         
Predicted:  i will s it in the parlor awhile and clect my thoughts 

Original:  but living soul there could be none to meet                     
Predicted:  bu living soul there could be none to meet 

Original:  the private could but he was no general you see                 
Predicted:  the priveate od but he was o janelasi 

Original:  the savage philosopher the dual mind                            
Predicted:  the savage philosopher the duel mind 

Original:  an everlasting laugh                                            
Predicted:  an ever lasting laugh 

Original:  this is a mistake though a perfectly natural one                
Predicted:  this isa mistake tog a perfectly natural one 

Original:  chapter thirty three a confidant                                
Predicted:  chapter thirty three a contfidant 

Original:  the sick man raged and shook his fist                           
Predicted:  the sick man raged and shook his fise 

Original:  it seemed as if his family troubles were just beginning         
Predicted:  it seemed as if his family troubles were just beganning 

Original:  is papa alone enquired miss temple                              
Predicted:  is up aon inquired mis timple 

Original:  the report is premature my good friend                          
Predicted:  the report is prema shor mi good friend 

Original:  doctor we all have our crosses have we not                      
Predicted:  doctor we all heave or crosses have we not 

Original:  it is like a bandage over one's eyes to come into it            
Predicted:  it it like abandage over one's eyes to come into it 

Original:  not at the hotel just now                                       
Predicted:  not at the hotel just now 

Original:  chapter four the first night in camp                            
Predicted:  chapter for the first niht in camo 

Original:  our pickets had run in and reported a night attack              
Predicted:  ore picket sad run in and reported a night attac 

Original:  spring came and passed and then summer                          
Predicted:  spring came and past and then summer 

Original:  the sharp smell of spirits went through the room                
Predicted:  the sharp smell of spirits went through the room 

Original:  no one could escape from this rictus                            
Predicted:  no and could escape from this rictus 

Original:  illustration marjoram                                           
Predicted:  illistration margerom 

Original:  bring it to the public attention dramatize it                   
Predicted:  bring it to the poblic attention dramatiseit 

Original:  i haven't courage enough to do it for myself                    
Predicted:  i have en courage enough to do it from myself 

Original:  pray go to bed for i am sure you must need rest                 
Predicted:  prea go to bed for i am sure you must need rest 

Original:  ursus and homo took charge of each other                        
Predicted:  erces an homotof charge of each other 

Original:  there was an extraordinary force of suggestion in this posturing
Predicted:  theugh was nestrornoy force of suggesten in this postring 

Original:  i don't say that protested michaelis gently                     
Predicted:  i don't say that protested my calis gently 

Original:  send my soul clean to asura                                     
Predicted:  send my soul qeean toasura 

Original:  they were within a few miles of their village now               
Predicted:  the were within a few miles of their village nowl 

Original:  i gasped positively gasped                                      
Predicted:  i guesd positibly gesspd 

Original:  the queen gazed upon our friends with evident interest          
Predicted:  the queen gasd upon our friends wth evitet interest 

Original:  lemon juice may be added at pleasure                            
Predicted:  lemon juce ma be added at pleasure 

Original:  what alternative was there for her                              
Predicted:  wat oltranit ove les there for her 

Original:  he no doubt would bring food of some kind with him              
Predicted:  he now det wild bring fod of somekind with ime 

Original:  bad generalship i thought it was christmas                      
Predicted:  bad genlship y foght it was christmasa 

Original:  fortunately there was nothing from his wife either              
Predicted:  ertunately there was nothing thorm his wie either 

Original:  but kirkland kept steadily on for the river                     
Predicted:  but carland kept steadly on for the river 

Log file saved:  results/1805_blstm_64xfull_360_05-18_1227.csv
Model saved:  models/1805_blstm_64xfull_360
Ending time:  2018-05-20 10:37:45
