/home/anitakau/envs/tensorflow-workq/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
WARNING:tensorflow:From /home/anitakau/envs/tensorflow-workq/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
2018-05-22 08:29:29.595005: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-22 08:29:31.122495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:03:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-22 08:29:31.283035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:82:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-22 08:29:31.283102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0, 1
2018-05-22 08:29:36.222500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-22 08:29:36.222566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 
2018-05-22 08:29:36.222575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N 
2018-05-22 08:29:36.222579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N 
2018-05-22 08:29:36.223249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15133 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 6.0)
2018-05-22 08:29:36.372849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15133 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0, compute capability: 6.0)

Reading training data:
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-train-clean-360.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 104014)
('Total number of files (after reduction):', 102100)
('max_intseq_length:', 280)
('numclasses:', 29)
('max_trans_charlength:', 280)
('Words:', 3490549)
('Vocab:', 59235)

Reading validation data: 
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-dev-clean.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 2703)
('Total number of files (after reduction):', 2616)
('max_intseq_length:', 279)
('numclasses:', 29)
('max_trans_charlength:', 279)
('Words:', 48972)
('Vocab:', 7726)

Reading test data: 
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-test-clean.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 2620)
('Total number of files (after reduction):', 2526)
('max_intseq_length:', 280)
('numclasses:', 29)
('max_trans_charlength:', 280)
('Words:', 46839)
('Vocab:', 7547)


Model and training parameters: 
Starting time:  2018-05-22 08:29:20
 - epochs:  60 
 - batch size:  64 
 - input epoch length:  0 
 - network epoch length:  1595 
 - training on  102080  files 
 - learning rate:  0.0001 
 - hidden units:  512 
 - mfcc features:  26 
 - dropout:  0.2 

Creating new model:  dnn_blstm
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
the_input (InputLayer)          (None, None, 26)     0                                            
__________________________________________________________________________________________________
fc_1 (TimeDistributed)          (None, None, 512)    13824       the_input[0][0]                  
__________________________________________________________________________________________________
dropout_1 (TimeDistributed)     (None, None, 512)    0           fc_1[0][0]                       
__________________________________________________________________________________________________
fc_2 (TimeDistributed)          (None, None, 512)    262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (TimeDistributed)     (None, None, 512)    0           fc_2[0][0]                       
__________________________________________________________________________________________________
fc_3 (TimeDistributed)          (None, None, 512)    262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dropout_3 (TimeDistributed)     (None, None, 512)    0           fc_3[0][0]                       
__________________________________________________________________________________________________
CuDNN_bi_lstm1 (Bidirectional)  (None, None, 512)    4202496     dropout_3[0][0]                  
__________________________________________________________________________________________________
CuDNN_bi_lstm2 (Bidirectional)  (None, None, 512)    4202496     CuDNN_bi_lstm1[0][0]             
__________________________________________________________________________________________________
fc_4 (TimeDistributed)          (None, None, 512)    262656      CuDNN_bi_lstm2[0][0]             
__________________________________________________________________________________________________
dropout_4 (TimeDistributed)     (None, None, 512)    0           fc_4[0][0]                       
__________________________________________________________________________________________________
softmax (TimeDistributed)       (None, None, 29)     14877       dropout_4[0][0]                  
__________________________________________________________________________________________________
the_labels (InputLayer)         (None, None)         0                                            
__________________________________________________________________________________________________
input_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
label_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    
                                                                 the_labels[0][0]                 
                                                                 input_length[0][0]               
                                                                 label_length[0][0]               
==================================================================================================
Total params: 9,221,661
Trainable params: 9,221,661
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/60
 - 3449s - loss: 391.5626 - val_loss: 121.2066

Epoch 00001: val_loss improved from inf to 121.20663, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.886997767857 

Epoch 2/60
 - 2406s - loss: 199.7808 - val_loss: 88.4917

Epoch 00002: val_loss improved from 121.20663 to 88.49171, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.797860863095 

Epoch 3/60
 - 2400s - loss: 155.7008 - val_loss: 72.4715

Epoch 00003: val_loss improved from 88.49171 to 72.47150, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.717931547619 

Epoch 4/60
 - 2396s - loss: 130.7569 - val_loss: 63.1138

Epoch 00004: val_loss improved from 72.47150 to 63.11376, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.680226934524 

Epoch 5/60
 - 2393s - loss: 114.0288 - val_loss: 56.5064

Epoch 00005: val_loss improved from 63.11376 to 56.50635, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.642764136905 

Epoch 6/60
 - 2394s - loss: 101.7678 - val_loss: 51.1331

Epoch 00006: val_loss improved from 56.50635 to 51.13308, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.605375744048 

Epoch 7/60
 - 2393s - loss: 92.3623 - val_loss: 47.0833

Epoch 00007: val_loss improved from 51.13308 to 47.08331, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.547023809524 

Epoch 8/60
 - 2391s - loss: 84.7585 - val_loss: 43.7390

Epoch 00008: val_loss improved from 47.08331 to 43.73901, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.508351934524 

Epoch 9/60
 - 2391s - loss: 78.6261 - val_loss: 41.4083

Epoch 00009: val_loss improved from 43.73901 to 41.40832, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.52658110119 

Epoch 10/60
 - 2388s - loss: 73.3573 - val_loss: 39.3032

Epoch 00010: val_loss improved from 41.40832 to 39.30320, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.480319940476 

Log file saved:  log_files/2205-2BLSTM-360_05-22_0829.csv
Epoch 11/60
 - 2391s - loss: 68.8619 - val_loss: 37.8982

Epoch 00011: val_loss improved from 39.30320 to 37.89822, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.460305059524 

Epoch 12/60
 - 2388s - loss: 64.8754 - val_loss: 36.3895

Epoch 00012: val_loss improved from 37.89822 to 36.38947, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.474330357143 

Epoch 13/60
 - 2388s - loss: 61.4714 - val_loss: 34.8540

Epoch 00013: val_loss improved from 36.38947 to 34.85400, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.444475446429 

Epoch 14/60
 - 2390s - loss: 58.4077 - val_loss: 33.5467

Epoch 00014: val_loss improved from 34.85400 to 33.54669, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.466629464286 

Epoch 15/60
 - 2385s - loss: 55.5683 - val_loss: 33.1637

Epoch 00015: val_loss improved from 33.54669 to 33.16367, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.472135416667 

Epoch 16/60
 - 2384s - loss: 53.1005 - val_loss: 32.0214

Epoch 00016: val_loss improved from 33.16367 to 32.02138, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.422321428571 

Epoch 17/60
 - 2384s - loss: 50.7703 - val_loss: 31.4155

Epoch 00017: val_loss improved from 32.02138 to 31.41555, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.465438988095 

Epoch 18/60
 - 2382s - loss: 48.6792 - val_loss: 30.3949

Epoch 00018: val_loss improved from 31.41555 to 30.39493, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.42345610119 

Epoch 19/60
 - 2381s - loss: 46.7936 - val_loss: 30.1578

Epoch 00019: val_loss improved from 30.39493 to 30.15775, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.401915922619 

Epoch 20/60
 - 2380s - loss: 45.0303 - val_loss: 29.6294

Epoch 00020: val_loss improved from 30.15775 to 29.62941, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.402399553571 

Log file saved:  log_files/2205-2BLSTM-360_05-22_0829.csv
Epoch 21/60
 - 2382s - loss: 43.3502 - val_loss: 29.2430

Epoch 00021: val_loss improved from 29.62941 to 29.24299, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.415048363095 

Epoch 22/60
 - 2379s - loss: 41.8098 - val_loss: 28.9890

Epoch 00022: val_loss improved from 29.24299 to 28.98900, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.386086309524 

Epoch 23/60
 - 2381s - loss: 40.3573 - val_loss: 28.2396

Epoch 00023: val_loss improved from 28.98900 to 28.23965, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.397414434524 

Epoch 24/60
 - 2377s - loss: 38.9828 - val_loss: 28.2482

Epoch 00024: val_loss did not improve
 - average WER:  0.403441220238 

Epoch 25/60
 - 2377s - loss: 37.7303 - val_loss: 27.7863

Epoch 00025: val_loss improved from 28.23965 to 27.78630, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.35232514881 

Epoch 26/60
 - 2376s - loss: 36.5220 - val_loss: 27.6336

Epoch 00026: val_loss improved from 27.78630 to 27.63361, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.376004464286 

Epoch 27/60
 - 2378s - loss: 35.3871 - val_loss: 27.9109

Epoch 00027: val_loss did not improve
 - average WER:  0.382142857143 

Epoch 28/60
 - 2376s - loss: 34.2966 - val_loss: 26.9787

Epoch 00028: val_loss improved from 27.63361 to 26.97872, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.409523809524 

Epoch 29/60
 - 2380s - loss: 33.2542 - val_loss: 26.7858

Epoch 00029: val_loss improved from 26.97872 to 26.78581, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.364936755952 

Epoch 30/60
 - 2376s - loss: 32.2981 - val_loss: 26.7862

Epoch 00030: val_loss did not improve
 - average WER:  0.389229910714 

Log file saved:  log_files/2205-2BLSTM-360_05-22_0829.csv
Epoch 31/60
 - 2375s - loss: 31.3854 - val_loss: 26.9021

Epoch 00031: val_loss did not improve
 - average WER:  0.383072916667 

Epoch 32/60
 - 2372s - loss: 30.4770 - val_loss: 26.3236

Epoch 00032: val_loss improved from 26.78581 to 26.32360, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.411960565476 

Epoch 33/60
 - 2374s - loss: 29.6896 - val_loss: 27.1336

Epoch 00033: val_loss did not improve
 - average WER:  0.374739583333 

Epoch 34/60
 - 2372s - loss: 28.8850 - val_loss: 26.5981

Epoch 00034: val_loss did not improve
 - average WER:  0.397898065476 

Epoch 35/60
 - 2379s - loss: 28.0769 - val_loss: 26.2743

Epoch 00035: val_loss improved from 26.32360 to 26.27432, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.41953125 

Epoch 36/60
 - 2374s - loss: 27.4081 - val_loss: 25.8603

Epoch 00036: val_loss improved from 26.27432 to 25.86029, saving model to modelssave/2205-2BLSTM-360_best
 - average WER:  0.365327380952 

Epoch 37/60
 - 2371s - loss: 26.6847 - val_loss: 26.7893

Epoch 00037: val_loss did not improve
 - average WER:  0.387890625 

Epoch 38/60
 - 2371s - loss: 26.1076 - val_loss: 26.4198

Epoch 00038: val_loss did not improve
 - average WER:  0.466313244048 

Epoch 39/60
 - 2377s - loss: 25.4337 - val_loss: 26.5938

Epoch 00039: val_loss did not improve
 - average WER:  0.424107142857 

Epoch 40/60
 - 2370s - loss: 24.7461 - val_loss: 26.4785

Epoch 00040: val_loss did not improve
 - average WER:  0.436402529762 

Log file saved:  log_files/2205-2BLSTM-360_05-22_0829.csv
Epoch 41/60
 - 2371s - loss: 24.1706 - val_loss: 26.4366

Epoch 00041: val_loss did not improve
 - average WER:  0.43515625 

Epoch 42/60
 - 2371s - loss: 23.5751 - val_loss: 26.8052

Epoch 00042: val_loss did not improve
 - average WER:  0.41056547619 

Epoch 43/60
 - 2377s - loss: 23.0818 - val_loss: 26.7410

Epoch 00043: val_loss did not improve
 - average WER:  0.42421875 

Epoch 44/60
 - 2371s - loss: 22.5387 - val_loss: 26.8528

Epoch 00044: val_loss did not improve
 - average WER:  0.391517857143 

Epoch 45/60
 - 2372s - loss: 22.0327 - val_loss: 26.7979

Epoch 00045: val_loss did not improve
 - average WER:  0.497953869048 

Epoch 46/60
 - 2369s - loss: 21.5749 - val_loss: 26.7081

Epoch 00046: val_loss did not improve
 - average WER:  0.430654761905 

Epoch 47/60
 - 2374s - loss: 21.0902 - val_loss: 27.2232

Epoch 00047: val_loss did not improve
 - average WER:  0.446614583333 

Epoch 48/60
 - 2370s - loss: 20.5557 - val_loss: 26.6100

Epoch 00048: val_loss did not improve
 - average WER:  0.438430059524 

Epoch 49/60
 - 2368s - loss: 20.1972 - val_loss: 26.8131

Epoch 00049: val_loss did not improve
 - average WER:  0.43634672619 

Epoch 50/60
 - 2371s - loss: 19.7247 - val_loss: 27.6274

Epoch 00050: val_loss did not improve
 - average WER:  0.444642857143 

Log file saved:  log_files/2205-2BLSTM-360_05-22_0829.csv
Epoch 51/60
 - 2367s - loss: 19.2726 - val_loss: 27.4330

Epoch 00051: val_loss did not improve
 - average WER:  0.427120535714 

Epoch 52/60
 - 2369s - loss: 18.9251 - val_loss: 27.4355

Epoch 00052: val_loss did not improve
 - average WER:  0.456770833333 

Epoch 53/60
 - 2367s - loss: 18.5525 - val_loss: 27.3967

Epoch 00053: val_loss did not improve
 - average WER:  0.437630208333 

Epoch 54/60
 - 2372s - loss: 18.1698 - val_loss: 28.1283

Epoch 00054: val_loss did not improve
 - average WER:  0.402864583333 

Epoch 55/60
 - 2370s - loss: 17.7853 - val_loss: 28.3064

Epoch 00055: val_loss did not improve
 - average WER:  0.407552083333 

Epoch 56/60
 - 2369s - loss: 17.4472 - val_loss: 27.9709

Epoch 00056: val_loss did not improve
 - average WER:  0.430357142857 

Epoch 57/60
 - 2367s - loss: 17.1205 - val_loss: 28.1048

Epoch 00057: val_loss did not improve
 - average WER:  0.422023809524 

Epoch 58/60
 - 2373s - loss: 16.8337 - val_loss: 28.2626

Epoch 00058: val_loss did not improve
 - average WER:  0.42619047619 

Epoch 59/60
 - 2367s - loss: 16.4991 - val_loss: 28.4170

Epoch 00059: val_loss did not improve
 - average WER:  0.404594494048 

Epoch 60/60
 - 2370s - loss: 16.1155 - val_loss: 28.4226

Epoch 00060: val_loss did not improve
 - average WER:  0.42150297619 

Log file saved:  log_files/2205-2BLSTM-360_05-22_0829.csv

 - Training ended, test wer:  0.369921875  -

Prediction samples:

Original:  he was stone dead but i dropped that foot quick                 
Predicted:  i was stone dead but i dropped at footquick 

Original:  an everlasting laugh                                            
Predicted:  an ever lasting laugh 

Original:  in about an hour and three quarters the boy returned            
Predicted:  in about an ou and three quorters the boay returned 

Original:  he could hardly realise how it had all come about               
Predicted:  he could hardly realize how it ad a'll come about 

Original:  but living soul there could be none to meet                     
Predicted:  but living soul there could be none to meet 

Original:  impertinent young beggar said burgess                           
Predicted:  impertinent young beggar said burgis 

Original:  i will sit in the parlor awhile and collect my thoughts         
Predicted:  i will sit in the parlar a while and collect my thoughts 

Original:  is papa alone enquired miss temple                              
Predicted:  is pap a lone inquired miss temple 

Original:  with that came a pang of intense pain                           
Predicted:  with that came a pang of intense pain 

Original:  boats put out both from the fort and the shore                  
Predicted:  boats put out both from the ford and the shore 

Original:  fortunately there was nothing from his wife either              
Predicted:  utginately the was nothing from is whife either 

Original:  i gasped positively gasped                                      
Predicted:  i gasped pasi tibly guaspedsc 

Original:  illustration marjoram                                           
Predicted:  illustration marjrum 

Original:  then bozzle came forward and introduced his wife                
Predicted:  then bazzo came forward an introduced his wife 

Original:  illustration rusks                                              
Predicted:  illustration rusksc 

Original:  what alternative was there for her                              
Predicted:  what altrndit o less there for her 

Original:  he also thought of his managerial position                      
Predicted:  he also thought of his man as oureal position 

Original:  i'm going to do what you asked me to do when you were in london 
Predicted:  i'm going to do you asked e to dou when you were in london 

Original:  i do not know confessed shaggy                                  
Predicted:  i do not know confess shaggy 

Original:  bring it to the public attention dramatize it                   
Predicted:  bring it to the poblic attention draotise it 

Original:  lemon juice may be added at pleasure                            
Predicted:  lemon juice me be added at pleasure 

Original:  they were within a few miles of their village now               
Predicted:  they were within a few miles of their village nowl 

Original:  to make good home made bread                                    
Predicted:  to make good home made breadet 

Original:  this is a mistake though a perfectly natural one                
Predicted:  this is a mistake tho perfectly natural one 

Original:  but kirkland kept steadily on for the river                     
Predicted:  but cerklin kept steadily on for the river 

Original:  ursus and homo took charge of each other                        
Predicted:  ercits an homo to charge of each other 

Original:  the report is premature my good friend                          
Predicted:  the report is preaa chore my good friendr 

Original:  illustration ginger                                             
Predicted:  illustration ginger 

Original:  spring came and passed and then summer                          
Predicted:  spring came in passe and then summerr 

Original:  at all events we shall get there some day                       
Predicted:  at all events we shall get there some daye 

Original:  but clews there are absolutely none                             
Predicted:  but cluse there are absolutely none 

Original:  it seemed as if his family troubles were just beginning         
Predicted:  it seemed as if his family troubles were just beginning 

Original:  couldn't help it then how can i ever trust you                  
Predicted:  couldn't help it then how can i ever trustio 

Original:  pray go to bed for i am sure you must need rest                 
Predicted:  pray go to bed for i am sure you must need restec 

Original:  the sharp smell of spirits went through the room                
Predicted:  the sharp smell of spirits went through the rom 

Original:  the savage philosopher the dual mind                            
Predicted:  the savage pilosopher the duil mindi 

Original:  bad generalship i thought it was christmas                      
Predicted:  bad general ship by phot it was christmas 

Original:  rosecrans protested it was in vain                              
Predicted:  rose crans protested it was in vainc 

Original:  doctor we all have our crosses have we not                      
Predicted:  doctor we all heve he crosses have wre not 

Original:  i haven't courage enough to do it for myself                    
Predicted:  i haven't courage enough to do it for myselfr 

Original:  the queen gazed upon our friends with evident interest          
Predicted:  the queen gazed upon our friends withevitdent interest 

Original:  i don't say that protested michaelis gently                     
Predicted:  i don't say that protested my callast  gentlyc 

Original:  chapter four the first night in camp                            
Predicted:  chepter for the first nigh en camp 

Original:  i made a quick glance over my shoulder and grabbed at my gun    
Predicted:  i made a quick glance or mys shoulder and grab thatmy gunc 

Original:  our pickets had run in and reported a night attack              
Predicted:  our pickets said run in ind reported a night attackr 

Original:  no one could escape from this rictus                            
Predicted:  no one could escape from this ricteuss 

Original:  there was an extraordinary force of suggestion in this posturing
Predicted:  there was anxtraordinoy force of suggestion in this posturing 

Original:  send my soul clean to asura                                     
Predicted:  send my so clean twas suras 

Original:  i was quartered with him at sarah island                        
Predicted:  i was quarterd with him at sarra islande 

Original:  it is like a bandage over one's eyes to come into it            
Predicted:  it did like a bandage over one's eyes to come into itc 

Original:  watch the savages outside said robert                           
Predicted:  watch the saaes outside said robertr 

Original:  the sick man raged and shook his fist                           
Predicted:  the sick man raged and shook his fist 

Original:  i had the pleasure of meeting him in society                    
Predicted:  i had the pleasure of meeting him in society 

Original:  chapter thirty three a confidant                                
Predicted:  chapter thirty three a contidatc 

Original:  must stop that fifty lashes troke                               
Predicted:  must stop that fifty lashe's troker 

Original:  for ambrosch my mama come here                                  
Predicted:  for ambroch m monmal come here 

Original:  not at the hotel just now                                       
Predicted:  not at the hotel just nowe 

Original:  i'll be glad to try sir he replied                              
Predicted:  i'll be glad to try sir ye repliedsc 

Original:  he pulled up a window as if the air were heavy                  
Predicted:  he pulled up a window as if the ar were heavyc 

Original:  yes and a very respectable one                                  
Predicted:  yes and a very respectable one 

Original:  he seemed to be cursing people who had wronged him              
Predicted:  he seemed tbe cursing people would wrong him 

Original:  sweetwater help me out of this                                  
Predicted:  sweet water phelte me out of thisc 

Original:  the private could but he was no general you see                 
Predicted:  the private good but he was no geneal yo see 

Original:  he no doubt would bring food of some kind with him              
Predicted:  he kno deubt would bring food of some kind withimg 

Log file saved:  log_files/2205-2BLSTM-360_05-22_0829.csv
Model saved:  modelssave/2205-2BLSTM-360
Ending time:  2018-05-24 00:27:26
