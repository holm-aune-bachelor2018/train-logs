/home/anitakau/envs/tensorflow-workq/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-05-15 12:33:56.748705: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-15 12:33:58.294707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:03:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-15 12:33:58.481230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:82:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-15 12:33:58.481666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0, 1
2018-05-15 12:34:04.018821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-15 12:34:04.018865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 
2018-05-15 12:34:04.018873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N 
2018-05-15 12:34:04.018877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N 
2018-05-15 12:34:04.019467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15133 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 6.0)
2018-05-15 12:34:04.173698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15133 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0, compute capability: 6.0)

Reading training data:
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-train-clean-360.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 104014)
('Total number of files (after reduction):', 102100)
('max_intseq_length:', 280)
('numclasses:', 29)
('max_trans_charlength:', 280)
('Words:', 3490549)
('Vocab:', 59235)

Reading validation data: 
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-dev-clean.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 2703)
('Total number of files (after reduction):', 2616)
('max_intseq_length:', 279)
('numclasses:', 29)
('max_trans_charlength:', 279)
('Words:', 48972)
('Vocab:', 7726)

Reading test data: 
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-test-clean.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 2620)
('Total number of files (after reduction):', 2526)
('max_intseq_length:', 280)
('numclasses:', 29)
('max_trans_charlength:', 280)
('Words:', 46839)
('Vocab:', 7547)


Model and training parameters: 
Starting time:  2018-05-15 12:33:54
 - epochs:  100 
 - batch size:  64 
 - input epoch length:  1024 
 - network epoch length:  1024 
 - training on  65536  files 
 - learning rate:  0.0001 
 - hidden units:  512 
 - mfcc features:  26 
 - dropout:  0.2 

Creating new model:  dnn_brnn
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
the_input (InputLayer)          (None, None, 26)     0                                            
__________________________________________________________________________________________________
masking (Masking)               (None, None, 26)     0           the_input[0][0]                  
__________________________________________________________________________________________________
fc_1 (TimeDistributed)          (None, None, 512)    13824       masking[0][0]                    
__________________________________________________________________________________________________
dropout_1 (TimeDistributed)     (None, None, 512)    0           fc_1[0][0]                       
__________________________________________________________________________________________________
fc_2 (TimeDistributed)          (None, None, 512)    262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (TimeDistributed)     (None, None, 512)    0           fc_2[0][0]                       
__________________________________________________________________________________________________
fc_3 (TimeDistributed)          (None, None, 512)    262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dropout_3 (TimeDistributed)     (None, None, 512)    0           fc_3[0][0]                       
__________________________________________________________________________________________________
bi_rnn1 (Bidirectional)         (None, None, 1024)   1049600     dropout_3[0][0]                  
__________________________________________________________________________________________________
bi_rnn2 (Bidirectional)         (None, None, 1024)   1573888     bi_rnn1[0][0]                    
__________________________________________________________________________________________________
fc_4 (TimeDistributed)          (None, None, 512)    524800      bi_rnn2[0][0]                    
__________________________________________________________________________________________________
dropout_4 (TimeDistributed)     (None, None, 512)    0           fc_4[0][0]                       
__________________________________________________________________________________________________
softmax (TimeDistributed)       (None, None, 29)     14877       dropout_4[0][0]                  
__________________________________________________________________________________________________
the_labels (InputLayer)         (None, None)         0                                            
__________________________________________________________________________________________________
input_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
label_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    
                                                                 the_labels[0][0]                 
                                                                 input_length[0][0]               
                                                                 label_length[0][0]               
==================================================================================================
Total params: 3,702,301
Trainable params: 3,702,301
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 - 3434s - loss: 493.5291 - val_loss: 200.9611

Epoch 00001: val_loss improved from inf to 200.96115, saving model to modelssave/1505_2brnn_best
 - average WER:  0.954482886905 

Epoch 2/100
 - 3341s - loss: 273.0713 - val_loss: 150.8096

Epoch 00002: val_loss improved from 200.96115 to 150.80961, saving model to modelssave/1505_2brnn_best
 - average WER:  0.864955357143 

Epoch 3/100
 - 3336s - loss: 225.7572 - val_loss: 123.9718

Epoch 00003: val_loss improved from 150.80961 to 123.97184, saving model to modelssave/1505_2brnn_best
 - average WER:  0.847061011905 

Epoch 4/100
 - 3336s - loss: 198.7996 - val_loss: 109.5950

Epoch 00004: val_loss improved from 123.97184 to 109.59498, saving model to modelssave/1505_2brnn_best
 - average WER:  0.842764136905 

Epoch 5/100
 - 3337s - loss: 180.4221 - val_loss: 97.9239

Epoch 00005: val_loss improved from 109.59498 to 97.92392, saving model to modelssave/1505_2brnn_best
 - average WER:  0.777492559524 

Epoch 6/100
 - 3335s - loss: 166.9081 - val_loss: 90.7970

Epoch 00006: val_loss improved from 97.92392 to 90.79700, saving model to modelssave/1505_2brnn_best
 - average WER:  0.767708333333 

Epoch 7/100
 - 3333s - loss: 156.4610 - val_loss: 85.3566

Epoch 00007: val_loss improved from 90.79700 to 85.35655, saving model to modelssave/1505_2brnn_best
 - average WER:  0.718229166667 

Epoch 8/100
 - 3338s - loss: 147.9205 - val_loss: 79.9054

Epoch 00008: val_loss improved from 85.35655 to 79.90538, saving model to modelssave/1505_2brnn_best
 - average WER:  0.728831845238 

Epoch 9/100
 - 3336s - loss: 140.8923 - val_loss: 76.4785

Epoch 00009: val_loss improved from 79.90538 to 76.47845, saving model to modelssave/1505_2brnn_best
 - average WER:  0.709337797619 

Epoch 10/100
 - 3335s - loss: 134.8540 - val_loss: 73.8862

Epoch 00010: val_loss improved from 76.47845 to 73.88621, saving model to modelssave/1505_2brnn_best
 - average WER:  0.673177083333 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 11/100
 - 3336s - loss: 129.6808 - val_loss: 71.3698

Epoch 00011: val_loss improved from 73.88621 to 71.36978, saving model to modelssave/1505_2brnn_best
 - average WER:  0.701711309524 

Epoch 12/100
 - 3330s - loss: 125.0587 - val_loss: 68.5057

Epoch 00012: val_loss improved from 71.36978 to 68.50572, saving model to modelssave/1505_2brnn_best
 - average WER:  0.659616815476 

Epoch 13/100
 - 3338s - loss: 121.1492 - val_loss: 66.4876

Epoch 00013: val_loss improved from 68.50572 to 66.48755, saving model to modelssave/1505_2brnn_best
 - average WER:  0.663002232143 

Epoch 14/100
 - 3334s - loss: 117.4094 - val_loss: 64.7061

Epoch 00014: val_loss improved from 66.48755 to 64.70605, saving model to modelssave/1505_2brnn_best
 - average WER:  0.651469494048 

Epoch 15/100
 - 3333s - loss: 114.1636 - val_loss: 63.3597

Epoch 00015: val_loss improved from 64.70605 to 63.35967, saving model to modelssave/1505_2brnn_best
 - average WER:  0.678385416667 

Epoch 16/100
 - 3335s - loss: 111.2240 - val_loss: 62.3881

Epoch 00016: val_loss improved from 63.35967 to 62.38813, saving model to modelssave/1505_2brnn_best
 - average WER:  0.659486607143 

Epoch 17/100
 - 3333s - loss: 108.5445 - val_loss: 61.5899

Epoch 00017: val_loss improved from 62.38813 to 61.58991, saving model to modelssave/1505_2brnn_best
 - average WER:  0.653701636905 

Epoch 18/100
 - 3329s - loss: 106.0984 - val_loss: 59.3588

Epoch 00018: val_loss improved from 61.58991 to 59.35880, saving model to modelssave/1505_2brnn_best
 - average WER:  0.624534970238 

Epoch 19/100
 - 3337s - loss: 103.7345 - val_loss: 58.2422

Epoch 00019: val_loss improved from 59.35880 to 58.24221, saving model to modelssave/1505_2brnn_best
 - average WER:  0.659114583333 

Epoch 20/100
 - 3332s - loss: 101.7647 - val_loss: 56.4570

Epoch 00020: val_loss improved from 58.24221 to 56.45700, saving model to modelssave/1505_2brnn_best
 - average WER:  0.609654017857 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 21/100
 - 3338s - loss: 99.7159 - val_loss: 55.7581

Epoch 00021: val_loss improved from 56.45700 to 55.75808, saving model to modelssave/1505_2brnn_best
 - average WER:  0.59255952381 

Epoch 22/100
 - 3341s - loss: 97.8623 - val_loss: 54.8776

Epoch 00022: val_loss improved from 55.75808 to 54.87764, saving model to modelssave/1505_2brnn_best
 - average WER:  0.612741815476 

Epoch 23/100
 - 3341s - loss: 96.1982 - val_loss: 54.2860

Epoch 00023: val_loss improved from 54.87764 to 54.28601, saving model to modelssave/1505_2brnn_best
 - average WER:  0.587593005952 

Epoch 24/100
 - 3343s - loss: 94.6506 - val_loss: 53.9322

Epoch 00024: val_loss improved from 54.28601 to 53.93215, saving model to modelssave/1505_2brnn_best
 - average WER:  0.601748511905 

Epoch 25/100
 - 3339s - loss: 93.0511 - val_loss: 53.0593

Epoch 00025: val_loss improved from 53.93215 to 53.05933, saving model to modelssave/1505_2brnn_best
 - average WER:  0.556919642857 

Epoch 26/100
 - 3340s - loss: 91.6096 - val_loss: 52.1521

Epoch 00026: val_loss improved from 53.05933 to 52.15205, saving model to modelssave/1505_2brnn_best
 - average WER:  0.573232886905 

Epoch 27/100
 - 3341s - loss: 90.2973 - val_loss: 52.0574

Epoch 00027: val_loss improved from 52.15205 to 52.05743, saving model to modelssave/1505_2brnn_best
 - average WER:  0.571149553571 

Epoch 28/100
 - 3345s - loss: 89.0041 - val_loss: 50.5626

Epoch 00028: val_loss improved from 52.05743 to 50.56265, saving model to modelssave/1505_2brnn_best
 - average WER:  0.569828869048 

Epoch 29/100
 - 3344s - loss: 87.8808 - val_loss: 50.2724

Epoch 00029: val_loss improved from 50.56265 to 50.27239, saving model to modelssave/1505_2brnn_best
 - average WER:  0.548828125 

Epoch 30/100
 - 3344s - loss: 86.6308 - val_loss: 49.7853

Epoch 00030: val_loss improved from 50.27239 to 49.78532, saving model to modelssave/1505_2brnn_best
 - average WER:  0.541685267857 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 31/100
 - 3339s - loss: 85.6081 - val_loss: 49.1763

Epoch 00031: val_loss improved from 49.78532 to 49.17631, saving model to modelssave/1505_2brnn_best
 - average WER:  0.522712053571 

Epoch 32/100
 - 3346s - loss: 84.4758 - val_loss: 48.8145

Epoch 00032: val_loss improved from 49.17631 to 48.81445, saving model to modelssave/1505_2brnn_best
 - average WER:  0.530543154762 

Epoch 33/100
 - 3337s - loss: 83.5028 - val_loss: 48.0905

Epoch 00033: val_loss improved from 48.81445 to 48.09050, saving model to modelssave/1505_2brnn_best
 - average WER:  0.513020833333 

Epoch 34/100
 - 3339s - loss: 82.4648 - val_loss: 48.0640

Epoch 00034: val_loss improved from 48.09050 to 48.06404, saving model to modelssave/1505_2brnn_best
 - average WER:  0.502901785714 

Epoch 35/100
 - 3341s - loss: 81.6130 - val_loss: 47.8705

Epoch 00035: val_loss improved from 48.06404 to 47.87050, saving model to modelssave/1505_2brnn_best
 - average WER:  0.55740327381 

Epoch 36/100
 - 3340s - loss: 80.6616 - val_loss: 46.7843

Epoch 00036: val_loss improved from 47.87050 to 46.78426, saving model to modelssave/1505_2brnn_best
 - average WER:  0.495777529762 

Epoch 37/100
 - 3343s - loss: 79.8512 - val_loss: 46.4029

Epoch 00037: val_loss improved from 46.78426 to 46.40286, saving model to modelssave/1505_2brnn_best
 - average WER:  0.516555059524 

Epoch 38/100
 - 3341s - loss: 79.0675 - val_loss: 46.1839

Epoch 00038: val_loss improved from 46.40286 to 46.18387, saving model to modelssave/1505_2brnn_best
 - average WER:  0.489490327381 

Epoch 39/100
 - 3341s - loss: 78.2332 - val_loss: 45.5989

Epoch 00039: val_loss improved from 46.18387 to 45.59890, saving model to modelssave/1505_2brnn_best
 - average WER:  0.499646577381 

Epoch 40/100
 - 3343s - loss: 77.4727 - val_loss: 45.4163

Epoch 00040: val_loss improved from 45.59890 to 45.41627, saving model to modelssave/1505_2brnn_best
 - average WER:  0.490792410714 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 41/100
 - 3341s - loss: 76.7090 - val_loss: 44.6428

Epoch 00041: val_loss improved from 45.41627 to 44.64278, saving model to modelssave/1505_2brnn_best
 - average WER:  0.479408482143 

Epoch 42/100
 - 3343s - loss: 76.0318 - val_loss: 45.3922

Epoch 00042: val_loss did not improve
 - average WER:  0.510770089286 

Epoch 43/100
 - 3340s - loss: 75.3306 - val_loss: 44.2816

Epoch 00043: val_loss improved from 44.64278 to 44.28158, saving model to modelssave/1505_2brnn_best
 - average WER:  0.480729166667 

Epoch 44/100
 - 3344s - loss: 74.6517 - val_loss: 44.3561

Epoch 00044: val_loss did not improve
 - average WER:  0.466164434524 

Epoch 45/100
 - 3339s - loss: 73.9363 - val_loss: 43.5436

Epoch 00045: val_loss improved from 44.28158 to 43.54360, saving model to modelssave/1505_2brnn_best
 - average WER:  0.490587797619 

Epoch 46/100
 - 3340s - loss: 73.4083 - val_loss: 43.3244

Epoch 00046: val_loss improved from 43.54360 to 43.32437, saving model to modelssave/1505_2brnn_best
 - average WER:  0.493787202381 

Epoch 47/100
 - 3343s - loss: 72.7136 - val_loss: 42.9673

Epoch 00047: val_loss improved from 43.32437 to 42.96727, saving model to modelssave/1505_2brnn_best
 - average WER:  0.466666666667 

Epoch 48/100
 - 3337s - loss: 72.1296 - val_loss: 42.7195

Epoch 00048: val_loss improved from 42.96727 to 42.71948, saving model to modelssave/1505_2brnn_best
 - average WER:  0.456863839286 

Epoch 49/100
 - 3341s - loss: 71.5715 - val_loss: 42.8873

Epoch 00049: val_loss did not improve
 - average WER:  0.46796875 

Epoch 50/100
 - 3335s - loss: 70.9040 - val_loss: 41.9399

Epoch 00050: val_loss improved from 42.71948 to 41.93995, saving model to modelssave/1505_2brnn_best
 - average WER:  0.497749255952 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 51/100
 - 3344s - loss: 70.4444 - val_loss: 42.0046

Epoch 00051: val_loss did not improve
 - average WER:  0.480710565476 

Epoch 52/100
 - 3339s - loss: 69.9768 - val_loss: 41.8739

Epoch 00052: val_loss improved from 41.93995 to 41.87389, saving model to modelssave/1505_2brnn_best
 - average WER:  0.467838541667 

Epoch 53/100
 - 3341s - loss: 69.3603 - val_loss: 41.8505

Epoch 00053: val_loss improved from 41.87389 to 41.85049, saving model to modelssave/1505_2brnn_best
 - average WER:  0.476450892857 

Epoch 54/100
 - 3342s - loss: 69.0156 - val_loss: 41.2121

Epoch 00054: val_loss improved from 41.85049 to 41.21211, saving model to modelssave/1505_2brnn_best
 - average WER:  0.462332589286 

Epoch 55/100
 - 3338s - loss: 68.4412 - val_loss: 41.3679

Epoch 00055: val_loss did not improve
 - average WER:  0.433519345238 

Epoch 56/100
 - 3337s - loss: 67.9538 - val_loss: 41.3225

Epoch 00056: val_loss did not improve
 - average WER:  0.476134672619 

Epoch 57/100
 - 3340s - loss: 67.5832 - val_loss: 40.7756

Epoch 00057: val_loss improved from 41.21211 to 40.77561, saving model to modelssave/1505_2brnn_best
 - average WER:  0.448158482143 

Epoch 58/100
 - 3337s - loss: 67.0374 - val_loss: 41.1787

Epoch 00058: val_loss did not improve
 - average WER:  0.485360863095 

Epoch 59/100
 - 3346s - loss: 66.6549 - val_loss: 40.5699

Epoch 00059: val_loss improved from 40.77561 to 40.56986, saving model to modelssave/1505_2brnn_best
 - average WER:  0.448121279762 

Epoch 60/100
 - 3341s - loss: 66.2286 - val_loss: 40.2229

Epoch 00060: val_loss improved from 40.56986 to 40.22294, saving model to modelssave/1505_2brnn_best
 - average WER:  0.399925595238 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 61/100
 - 3336s - loss: 65.7671 - val_loss: 39.8450

Epoch 00061: val_loss improved from 40.22294 to 39.84500, saving model to modelssave/1505_2brnn_best
 - average WER:  0.433389136905 

Epoch 62/100
 - 3339s - loss: 65.3968 - val_loss: 40.0389

Epoch 00062: val_loss did not improve
 - average WER:  0.405394345238 

Epoch 63/100
 - 3337s - loss: 64.9005 - val_loss: 39.6804

Epoch 00063: val_loss improved from 39.84500 to 39.68045, saving model to modelssave/1505_2brnn_best
 - average WER:  0.471484375 

Epoch 64/100
 - 3338s - loss: 64.5678 - val_loss: 40.0216

Epoch 00064: val_loss did not improve
 - average WER:  0.417782738095 

Epoch 65/100
 - 3336s - loss: 64.2779 - val_loss: 39.6897

Epoch 00065: val_loss did not improve
 - average WER:  0.441982886905 

Epoch 66/100
 - 3338s - loss: 63.8700 - val_loss: 39.2963

Epoch 00066: val_loss improved from 39.68045 to 39.29634, saving model to modelssave/1505_2brnn_best
 - average WER:  0.429482886905 

Epoch 67/100
 - 3336s - loss: 63.5494 - val_loss: 39.0276

Epoch 00067: val_loss improved from 39.29634 to 39.02761, saving model to modelssave/1505_2brnn_best
 - average WER:  0.436197916667 

Epoch 68/100
 - 3344s - loss: 63.1042 - val_loss: 39.5037

Epoch 00068: val_loss did not improve
 - average WER:  0.461532738095 

Epoch 69/100
 - 3340s - loss: 62.8041 - val_loss: 38.9847

Epoch 00069: val_loss improved from 39.02761 to 38.98468, saving model to modelssave/1505_2brnn_best
 - average WER:  0.434040178571 

Epoch 70/100
 - 3338s - loss: 62.4698 - val_loss: 39.0956

Epoch 00070: val_loss did not improve
 - average WER:  0.411104910714 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 71/100
 - 3337s - loss: 62.0753 - val_loss: 38.7541

Epoch 00071: val_loss improved from 38.98468 to 38.75413, saving model to modelssave/1505_2brnn_best
 - average WER:  0.431901041667 

Epoch 72/100
 - 3338s - loss: 61.8173 - val_loss: 38.7188

Epoch 00072: val_loss improved from 38.75413 to 38.71881, saving model to modelssave/1505_2brnn_best
 - average WER:  0.445814732143 

Epoch 73/100
 - 3350s - loss: 61.4885 - val_loss: 38.2346

Epoch 00073: val_loss improved from 38.71881 to 38.23464, saving model to modelssave/1505_2brnn_best
 - average WER:  0.395517113095 

Epoch 74/100
 - 3339s - loss: 61.1193 - val_loss: 38.4645

Epoch 00074: val_loss did not improve
 - average WER:  0.464267113095 

Epoch 75/100
 - 3343s - loss: 60.8970 - val_loss: 38.1539

Epoch 00075: val_loss improved from 38.23464 to 38.15391, saving model to modelssave/1505_2brnn_best
 - average WER:  0.416536458333 

Epoch 76/100
 - 3340s - loss: 60.4809 - val_loss: 38.1402

Epoch 00076: val_loss improved from 38.15391 to 38.14019, saving model to modelssave/1505_2brnn_best
 - average WER:  0.431287202381 

Epoch 77/100
 - 3343s - loss: 60.2061 - val_loss: 37.7198

Epoch 00077: val_loss improved from 38.14019 to 37.71983, saving model to modelssave/1505_2brnn_best
 - average WER:  0.388541666667 

Epoch 78/100
 - 3341s - loss: 59.9721 - val_loss: 38.1701

Epoch 00078: val_loss did not improve
 - average WER:  0.429929315476 

Epoch 79/100
 - 3340s - loss: 59.6948 - val_loss: 38.0724

Epoch 00079: val_loss did not improve
 - average WER:  0.418805803571 

Epoch 80/100
 - 3341s - loss: 59.3111 - val_loss: 37.4362

Epoch 00080: val_loss improved from 37.71983 to 37.43619, saving model to modelssave/1505_2brnn_best
 - average WER:  0.387109375 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 81/100
 - 3339s - loss: 58.9830 - val_loss: 37.1555

Epoch 00081: val_loss improved from 37.43619 to 37.15554, saving model to modelssave/1505_2brnn_best
 - average WER:  0.41755952381 

Epoch 82/100
 - 3343s - loss: 58.7687 - val_loss: 37.5138

Epoch 00082: val_loss did not improve
 - average WER:  0.407217261905 

Epoch 83/100
 - 3342s - loss: 58.5107 - val_loss: 37.5977

Epoch 00083: val_loss did not improve
 - average WER:  0.390401785714 

Epoch 84/100
 - 3344s - loss: 58.1987 - val_loss: 36.8844

Epoch 00084: val_loss improved from 37.15554 to 36.88444, saving model to modelssave/1505_2brnn_best
 - average WER:  0.358761160714 

Epoch 85/100
 - 3336s - loss: 57.9397 - val_loss: 37.3935

Epoch 00085: val_loss did not improve
 - average WER:  0.36052827381 

Epoch 86/100
 - 3340s - loss: 57.8312 - val_loss: 37.2862

Epoch 00086: val_loss did not improve
 - average WER:  0.413895089286 

Epoch 87/100
 - 3340s - loss: 57.5081 - val_loss: 36.9426

Epoch 00087: val_loss did not improve
 - average WER:  0.415383184524 

Epoch 88/100
 - 3339s - loss: 57.3446 - val_loss: 36.8841

Epoch 00088: val_loss improved from 36.88444 to 36.88413, saving model to modelssave/1505_2brnn_best
 - average WER:  0.417485119048 

Epoch 89/100
 - 3340s - loss: 57.1778 - val_loss: 36.6901

Epoch 00089: val_loss improved from 36.88413 to 36.69008, saving model to modelssave/1505_2brnn_best
 - average WER:  0.421223958333 

Epoch 90/100
 - 3343s - loss: 56.7777 - val_loss: 36.7693

Epoch 00090: val_loss did not improve
 - average WER:  0.385677083333 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Epoch 91/100
 - 3341s - loss: 56.6364 - val_loss: 37.0698

Epoch 00091: val_loss did not improve
 - average WER:  0.39376860119 

Epoch 92/100
 - 3342s - loss: 56.3773 - val_loss: 36.5831

Epoch 00092: val_loss improved from 36.69008 to 36.58310, saving model to modelssave/1505_2brnn_best
 - average WER:  0.393638392857 

Epoch 93/100
 - 3341s - loss: 56.2226 - val_loss: 36.5580

Epoch 00093: val_loss improved from 36.58310 to 36.55804, saving model to modelssave/1505_2brnn_best
 - average WER:  0.419847470238 

Epoch 94/100
 - 3342s - loss: 55.9873 - val_loss: 36.2168

Epoch 00094: val_loss improved from 36.55804 to 36.21677, saving model to modelssave/1505_2brnn_best
 - average WER:  0.38751860119 

Epoch 95/100
 - 3341s - loss: 55.6067 - val_loss: 36.1228

Epoch 00095: val_loss improved from 36.21677 to 36.12280, saving model to modelssave/1505_2brnn_best
 - average WER:  0.38203125 

Epoch 96/100
 - 3342s - loss: 55.4605 - val_loss: 36.1084

Epoch 00096: val_loss improved from 36.12280 to 36.10838, saving model to modelssave/1505_2brnn_best
 - average WER:  0.410305059524 

Epoch 97/100
 - 3338s - loss: 55.3249 - val_loss: 35.8507

Epoch 00097: val_loss improved from 36.10838 to 35.85070, saving model to modelssave/1505_2brnn_best
 - average WER:  0.398363095238 

Epoch 98/100
 - 3340s - loss: 55.0243 - val_loss: 36.3614

Epoch 00098: val_loss did not improve
 - average WER:  0.397526041667 

Epoch 99/100
 - 3339s - loss: 54.8765 - val_loss: 35.6673

Epoch 00099: val_loss improved from 35.85070 to 35.66733, saving model to modelssave/1505_2brnn_best
 - average WER:  0.394921875 

Epoch 100/100
 - 3342s - loss: 54.6011 - val_loss: 35.6770

Epoch 00100: val_loss did not improve
 - average WER:  0.404185267857 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv

 - Training ended, test wer:  0.414248511905  -

Prediction samples:
Original:  i don't say that protested michaelis gently                     
Predicted:  a don't say that protesten my calas trently 

Original:  impertinent young beggar said burgess                           
Predicted:  impertinate young peggor said purgus 

Original:  is papa alone enquired miss temple                              
Predicted:  is pap alone inqured mis temple 

Original:  the sick man raged and shook his fist                           
Predicted:  the sick man raged an shook his fist 

Original:  chapter thirty three a confidant                                
Predicted:  chapter thirty three a concidat 

Original:  ursus and homo took charge of each other                        
Predicted:  ersis an ho mot ov charge of each other 

Original:  this is a mistake though a perfectly natural one                
Predicted:  this is a mistake tnog perfectly natural one 

Original:  he seemed to be cursing people who had wronged him              
Predicted:  he seemd to be cursing peoplewold round him 

Original:  there was an extraordinary force of suggestion in this posturing
Predicted:  there was en stornay force of sugjusten in this postering 

Original:  they were within a few miles of their village now               
Predicted:  they were within a few miles of their billage naw 

Original:  to make good home made bread                                    
Predicted:  to make good holme made bread 

Original:  but kirkland kept steadily on for the river                     
Predicted:  but carlin kept steadily on for the river 

Original:  must stop that fifty lashes troke                               
Predicted:  mu stop that fifty lashes troke 

Original:  at all events we shall get there some day                       
Predicted:  at all ev ense we shall get their someday 

Original:  watch the savages outside said robert                           
Predicted:  watch teabigis outside said rover 

Original:  lemon juice may be added at pleasure                            
Predicted:  lemon juce may be added at pleasure 

Original:  not at the hotel just now                                       
Predicted:  not at the hotel just now 

Original:  the report is premature my good friend                          
Predicted:  the report ist premmathor my good friend 

Original:  with that came a pang of intense pain                           
Predicted:  with that came a paing of intense pam 

Original:  chapter four the first night in camp                            
Predicted:  chaper for the first nght in camp 

Original:  illustration marjoram                                           
Predicted:  illustration margerom 

Original:  illustration rusks                                              
Predicted:  eli stration reses 

Original:  an everlasting laugh                                            
Predicted:  and ever lasting laugh 

Original:  yes and a very respectable one                                  
Predicted:  yes and a vero respectable one 

Original:  couldn't help it then how can i ever trust you                  
Predicted:  could n' thel it thant how can i ever trustiol 

Original:  but clews there are absolutely none                             
Predicted:  but close there are absoutely non 

Original:  spring came and passed and then summer                          
Predicted:  spring came in past and then summer 

Original:  doctor we all have our crosses have we not                      
Predicted:  doctor we all hav er crosses have we not 

Original:  then bozzle came forward and introduced his wife                
Predicted:  then bosle came forward and introduced his wife 

Original:  i made a quick glance over my shoulder and grabbed at my gun    
Predicted:  i made a quick glance or ric houler and grabbed my gun 

Original:  i had the pleasure of meeting him in society                    
Predicted:  i had the pleasure of meeting him in society 

Original:  boats put out both from the fort and the shore                  
Predicted:  boats put ut both from the ford an the shore 

Original:  it is like a bandage over one's eyes to come into it            
Predicted:  it ad like abandage over one's eyes to come into it 

Original:  the queen gazed upon our friends with evident interest          
Predicted:  the queen gazed upon er friends with evitit interest 

Original:  sweetwater help me out of this                                  
Predicted:  sweetwater olfe me out of this 

Original:  pray go to bed for i am sure you must need rest                 
Predicted:  pra go to bed for i am sure you must need rest 

Original:  the savage philosopher the dual mind                            
Predicted:  the savage polosofher the dua mind 

Original:  he was stone dead but i dropped that foot quick                 
Predicted:  hi was stone deadd but i dropped at foot quick 

Original:  our pickets had run in and reported a night attack              
Predicted:  are picked said run in an reported ha nigt attack 

Original:  the sharp smell of spirits went through the room                
Predicted:  the sharp smell of spirits went through the rom 

Original:  send my soul clean to asura                                     
Predicted:  send my so clean to assura 

Original:  it seemed as if his family troubles were just beginning         
Predicted:  it seemd aes ef his fanly troubles were just beganning 

Original:  but living soul there could be none to meet                     
Predicted:  te living soul there could be none to meet 

Original:  he also thought of his managerial position                      
Predicted:  he alo thought of his minageriaposition 

Original:  i do not know confessed shaggy                                  
Predicted:  i do not kno convess chagy 

Original:  no one could escape from this rictus                            
Predicted:  no en could escape from this rictice 

Original:  bring it to the public attention dramatize it                   
Predicted:  bring i to the public attention dramitise it 

Original:  the private could but he was no general you see                 
Predicted:  the frived cooid but he was no gunelyousy 

Original:  he no doubt would bring food of some kind with him              
Predicted:  he kno debt would bring foit of some kind withim 

Original:  i gasped positively gasped                                      
Predicted:  i guest positibly gaesp 

Original:  i haven't courage enough to do it for myself                    
Predicted:  i haveent courage enough to do it form myself 

Original:  i'll be glad to try sir he replied                              
Predicted:  i'll be glad to tri sir ye replied 

Original:  i was quartered with him at sarah island                        
Predicted:  i was quarted with him at sera island 

Original:  fortunately there was nothing from his wife either              
Predicted:  fatinatly there was nothing from his wie either 

Original:  rosecrans protested it was in vain                              
Predicted:  rose cranse protested it was invain 

Original:  i will sit in the parlor awhile and collect my thoughts         
Predicted:  i will fit in the parler a while and colect my thoughts 

Original:  bad generalship i thought it was christmas                      
Predicted:  bad general ship i thouht it was christmas 

Original:  in about an hour and three quarters the boy returned            
Predicted:  in about i hour and three coarters the bo returnd 

Original:  for ambrosch my mama come here                                  
Predicted:  for ambroshe my amo come here 

Original:  illustration ginger                                             
Predicted:  illustration ginger 

Original:  what alternative was there for her                              
Predicted:  what alturnit of was therfore her 

Original:  i'm going to do what you asked me to do when you were in london 
Predicted:  i'nd going to do  you asked me did you when you were i london 

Original:  he could hardly realise how it had all come about               
Predicted:  he gold hardly realize how it at all come abot 

Original:  he pulled up a window as if the air were heavy                  
Predicted:  he pulled up a wondow as if the ear were heavy 

Log file saved:  log_files/1505_2brnn_05-15_1234.csv
Model saved:  modelssave/1505_2brnn
Ending time:  2018-05-19 09:24:20
