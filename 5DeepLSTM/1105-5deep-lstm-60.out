/home/anitakau/envs/tensorflow-workq/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
WARNING:tensorflow:From /home/anitakau/envs/tensorflow-workq/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
2018-05-11 08:49:56.811746: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-11 08:49:58.309130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:03:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-11 08:49:58.494678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:82:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-05-11 08:49:58.494754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0, 1
2018-05-11 08:50:03.415974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-05-11 08:50:03.416260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 1 
2018-05-11 08:50:03.416270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N N 
2018-05-11 08:50:03.416275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 1:   N N 
2018-05-11 08:50:03.417040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15133 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:03:00.0, compute capability: 6.0)
2018-05-11 08:50:03.573303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15133 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0, compute capability: 6.0)

Reading training data:
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-train-clean-360.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 104014)
('Total number of files (after reduction):', 102100)
('max_intseq_length:', 280)
('numclasses:', 29)
('max_trans_charlength:', 280)
('Words:', 3490549)
('Vocab:', 59235)

Reading validation data: 
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-dev-clean.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 2703)
('Total number of files (after reduction):', 2616)
('max_intseq_length:', 279)
('numclasses:', 29)
('max_trans_charlength:', 279)
('Words:', 48972)
('Vocab:', 7726)

Reading test data: 
('Reading csv:', '/home/anitakau/ctc/data_dir/librivox-test-clean.csv')
Finished reading in data
removing any sentences that are too big- tweetsize
('Total number of files:', 2620)
('Total number of files (after reduction):', 2526)
('max_intseq_length:', 280)
('numclasses:', 29)
('max_trans_charlength:', 280)
('Words:', 46839)
('Vocab:', 7547)


Model and training parameters: 
Starting time:  2018-05-11 08:49:45
 - epochs:  100 
 - batch size:  64 
 - input epoch length:  1024 
 - network epoch length:  1024 
 - training on  65536  files 
 - learning rate:  0.0001 
 - hidden units:  512 
 - mfcc features:  26 
 - dropout:  0.2 

Creating new model:  deep_lstm
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
the_input (InputLayer)          (None, None, 26)     0                                            
__________________________________________________________________________________________________
fc_1 (TimeDistributed)          (None, None, 512)    13824       the_input[0][0]                  
__________________________________________________________________________________________________
dropout_1 (TimeDistributed)     (None, None, 512)    0           fc_1[0][0]                       
__________________________________________________________________________________________________
fc_2 (TimeDistributed)          (None, None, 512)    262656      dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (TimeDistributed)     (None, None, 512)    0           fc_2[0][0]                       
__________________________________________________________________________________________________
fc_3 (TimeDistributed)          (None, None, 512)    262656      dropout_2[0][0]                  
__________________________________________________________________________________________________
dropout_3 (TimeDistributed)     (None, None, 512)    0           fc_3[0][0]                       
__________________________________________________________________________________________________
CuDNN_lstm1 (CuDNNLSTM)         (None, None, 512)    2101248     dropout_3[0][0]                  
__________________________________________________________________________________________________
CuDNN_lstm2 (CuDNNLSTM)         (None, None, 512)    2101248     CuDNN_lstm1[0][0]                
__________________________________________________________________________________________________
CuDNN_lstm3 (CuDNNLSTM)         (None, None, 512)    2101248     CuDNN_lstm2[0][0]                
__________________________________________________________________________________________________
CuDNN_lstm4 (CuDNNLSTM)         (None, None, 512)    2101248     CuDNN_lstm3[0][0]                
__________________________________________________________________________________________________
CuDNN_lstm5 (CuDNNLSTM)         (None, None, 512)    2101248     CuDNN_lstm4[0][0]                
__________________________________________________________________________________________________
fc_4 (TimeDistributed)          (None, None, 512)    262656      CuDNN_lstm5[0][0]                
__________________________________________________________________________________________________
dropout_4 (TimeDistributed)     (None, None, 512)    0           fc_4[0][0]                       
__________________________________________________________________________________________________
softmax (TimeDistributed)       (None, None, 29)     14877       dropout_4[0][0]                  
__________________________________________________________________________________________________
the_labels (InputLayer)         (None, None)         0                                            
__________________________________________________________________________________________________
input_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
label_length (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    
                                                                 the_labels[0][0]                 
                                                                 input_length[0][0]               
                                                                 label_length[0][0]               
==================================================================================================
Total params: 11,322,909
Trainable params: 11,322,909
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
 - 2111s - loss: 464.3077 - val_loss: 244.2596

Epoch 00001: val_loss improved from inf to 244.25963, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.992410714286
Epoch 2/100
 - 1774s - loss: 321.3621 - val_loss: 154.0924

Epoch 00002: val_loss improved from 244.25963 to 154.09239, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.97579985119
Epoch 3/100
 - 1770s - loss: 230.9445 - val_loss: 122.0093

Epoch 00003: val_loss improved from 154.09239 to 122.00932, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.965364583333
Epoch 4/100
 - 1765s - loss: 191.2158 - val_loss: 103.4683

Epoch 00004: val_loss improved from 122.00932 to 103.46832, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.913374255952
Epoch 5/100
 - 1767s - loss: 165.8328 - val_loss: 91.5073

Epoch 00005: val_loss improved from 103.46832 to 91.50734, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.900967261905
Epoch 6/100
 - 1764s - loss: 147.8417 - val_loss: 83.2808

Epoch 00006: val_loss improved from 91.50734 to 83.28082, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.857310267857
Epoch 7/100
 - 1765s - loss: 134.1718 - val_loss: 76.0507

Epoch 00007: val_loss improved from 83.28082 to 76.05067, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.850465029762
Epoch 8/100
 - 1763s - loss: 123.3430 - val_loss: 71.7518

Epoch 00008: val_loss improved from 76.05067 to 71.75182, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.840290178571
Epoch 9/100
 - 1762s - loss: 114.6123 - val_loss: 67.0884

Epoch 00009: val_loss improved from 71.75182 to 67.08841, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.81443452381
Epoch 10/100
 - 1761s - loss: 107.2983 - val_loss: 63.6512

Epoch 00010: val_loss improved from 67.08841 to 63.65120, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.825539434524
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 11/100
 - 1762s - loss: 101.1355 - val_loss: 59.9595

Epoch 00011: val_loss improved from 63.65120 to 59.95948, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.771409970238
Epoch 12/100
 - 1762s - loss: 95.6533 - val_loss: 57.1546

Epoch 00012: val_loss improved from 59.95948 to 57.15456, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.784728422619
Epoch 13/100
 - 1759s - loss: 91.0654 - val_loss: 55.2037

Epoch 00013: val_loss improved from 57.15456 to 55.20373, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.76482514881
Epoch 14/100
 - 1761s - loss: 86.8084 - val_loss: 53.1935

Epoch 00014: val_loss improved from 55.20373 to 53.19353, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.746130952381
Epoch 15/100
 - 1759s - loss: 83.1104 - val_loss: 50.8459

Epoch 00015: val_loss improved from 53.19353 to 50.84592, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.729724702381
Epoch 16/100
 - 1760s - loss: 79.8767 - val_loss: 49.6736

Epoch 00016: val_loss improved from 50.84592 to 49.67356, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.687965029762
Epoch 17/100
 - 1760s - loss: 76.6595 - val_loss: 47.7841

Epoch 00017: val_loss improved from 49.67356 to 47.78405, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.704148065476
Epoch 18/100
 - 1758s - loss: 73.9026 - val_loss: 47.6687

Epoch 00018: val_loss improved from 47.78405 to 47.66866, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.702883184524
Epoch 19/100
 - 1758s - loss: 71.1772 - val_loss: 45.6600

Epoch 00019: val_loss improved from 47.66866 to 45.65997, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.677194940476
Epoch 20/100
 - 1758s - loss: 68.9055 - val_loss: 44.8494

Epoch 00020: val_loss improved from 45.65997 to 44.84944, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.677101934524
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 21/100
 - 1757s - loss: 66.6138 - val_loss: 43.9261

Epoch 00021: val_loss improved from 44.84944 to 43.92609, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.67224702381
Epoch 22/100
 - 1756s - loss: 64.6488 - val_loss: 42.7145

Epoch 00022: val_loss improved from 43.92609 to 42.71451, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.664118303571
Epoch 23/100
 - 1756s - loss: 62.6565 - val_loss: 42.3689

Epoch 00023: val_loss improved from 42.71451 to 42.36887, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.672470238095
Epoch 24/100
 - 1758s - loss: 60.8762 - val_loss: 41.4091

Epoch 00024: val_loss improved from 42.36887 to 41.40915, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.659542410714
Epoch 25/100
 - 1757s - loss: 59.2241 - val_loss: 41.3996

Epoch 00025: val_loss improved from 41.40915 to 41.39958, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.665345982143
Epoch 26/100
 - 1756s - loss: 57.6923 - val_loss: 40.4845

Epoch 00026: val_loss improved from 41.39958 to 40.48445, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.641964285714
Epoch 27/100
 - 1756s - loss: 56.2584 - val_loss: 39.5853

Epoch 00027: val_loss improved from 40.48445 to 39.58532, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.649423363095
Epoch 28/100
 - 1757s - loss: 54.7020 - val_loss: 39.4061

Epoch 00028: val_loss improved from 39.58532 to 39.40610, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.623158482143
Epoch 29/100
 - 1755s - loss: 53.2591 - val_loss: 39.7140

Epoch 00029: val_loss did not improve
 - average WER:  0.652771577381
Epoch 30/100
 - 1755s - loss: 52.0004 - val_loss: 39.3320

Epoch 00030: val_loss improved from 39.40610 to 39.33197, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.62423735119
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 31/100
 - 1754s - loss: 50.8558 - val_loss: 38.9256

Epoch 00031: val_loss improved from 39.33197 to 38.92558, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.623158482143
Epoch 32/100
 - 1755s - loss: 49.5376 - val_loss: 38.7478

Epoch 00032: val_loss improved from 38.92558 to 38.74780, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.644047619048
Epoch 33/100
 - 1753s - loss: 48.5629 - val_loss: 37.7688

Epoch 00033: val_loss improved from 38.74780 to 37.76879, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.568545386905
Epoch 34/100
 - 1754s - loss: 47.3273 - val_loss: 37.7055

Epoch 00034: val_loss improved from 37.76879 to 37.70545, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.62459077381
Epoch 35/100
 - 1754s - loss: 46.3027 - val_loss: 37.1341

Epoch 00035: val_loss improved from 37.70545 to 37.13406, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.626674107143
Epoch 36/100
 - 1752s - loss: 45.4894 - val_loss: 37.1698

Epoch 00036: val_loss did not improve
 - average WER:  0.630301339286
Epoch 37/100
 - 1753s - loss: 44.5510 - val_loss: 37.3251

Epoch 00037: val_loss did not improve
 - average WER:  0.598883928571
Epoch 38/100
 - 1753s - loss: 43.5729 - val_loss: 37.4845

Epoch 00038: val_loss did not improve
 - average WER:  0.582161458333
Epoch 39/100
 - 1750s - loss: 42.6442 - val_loss: 36.5548

Epoch 00039: val_loss improved from 37.13406 to 36.55476, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.594661458333
Epoch 40/100
 - 1752s - loss: 41.6562 - val_loss: 36.6748

Epoch 00040: val_loss did not improve
 - average WER:  0.606324404762
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 41/100
 - 1752s - loss: 40.8167 - val_loss: 36.2530

Epoch 00041: val_loss improved from 36.55476 to 36.25300, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.593843005952
Epoch 42/100
 - 1752s - loss: 40.0954 - val_loss: 36.2937

Epoch 00042: val_loss did not improve
 - average WER:  0.606361607143
Epoch 43/100
 - 1752s - loss: 39.2044 - val_loss: 35.8863

Epoch 00043: val_loss improved from 36.25300 to 35.88634, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.594624255952
Epoch 44/100
 - 1752s - loss: 38.5168 - val_loss: 36.6020

Epoch 00044: val_loss did not improve
 - average WER:  0.576116071429
Epoch 45/100
 - 1750s - loss: 37.7561 - val_loss: 35.5106

Epoch 00045: val_loss improved from 35.88634 to 35.51064, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.568433779762
Epoch 46/100
 - 1748s - loss: 37.0283 - val_loss: 35.7532

Epoch 00046: val_loss did not improve
 - average WER:  0.600576636905
Epoch 47/100
 - 1751s - loss: 36.4250 - val_loss: 36.0271

Epoch 00047: val_loss did not improve
 - average WER:  0.585993303571
Epoch 48/100
 - 1747s - loss: 35.8423 - val_loss: 35.7164

Epoch 00048: val_loss did not improve
 - average WER:  0.623493303571
Epoch 49/100
 - 1751s - loss: 34.9946 - val_loss: 36.1149

Epoch 00049: val_loss did not improve
 - average WER:  0.555691964286
Epoch 50/100
 - 1749s - loss: 34.4249 - val_loss: 35.6630

Epoch 00050: val_loss did not improve
 - average WER:  0.589639136905
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 51/100
 - 1757s - loss: 33.6625 - val_loss: 35.6987

Epoch 00051: val_loss did not improve
 - average WER:  0.585788690476
Epoch 52/100
 - 1752s - loss: 33.1666 - val_loss: 35.5370

Epoch 00052: val_loss did not improve
 - average WER:  0.54998139881
Epoch 53/100
 - 1752s - loss: 32.5187 - val_loss: 35.2958

Epoch 00053: val_loss improved from 35.51064 to 35.29585, saving model to modelssave/1105-5deep-lstm-60_best
 - average WER:  0.587667410714
Epoch 54/100
 - 1749s - loss: 31.9401 - val_loss: 35.7520

Epoch 00054: val_loss did not improve
 - average WER:  0.550446428571
Epoch 55/100
 - 1749s - loss: 31.3238 - val_loss: 36.1626

Epoch 00055: val_loss did not improve
 - average WER:  0.580691964286
Epoch 56/100
 - 1749s - loss: 30.9308 - val_loss: 35.6696

Epoch 00056: val_loss did not improve
 - average WER:  0.564657738095
Epoch 57/100
 - 1747s - loss: 30.2662 - val_loss: 35.8963

Epoch 00057: val_loss did not improve
 - average WER:  0.532496279762
Epoch 58/100
 - 1749s - loss: 29.7421 - val_loss: 36.2266

Epoch 00058: val_loss did not improve
 - average WER:  0.577287946429
Epoch 59/100
 - 1747s - loss: 29.3779 - val_loss: 36.1408

Epoch 00059: val_loss did not improve
 - average WER:  0.56912202381
Epoch 60/100
 - 1768s - loss: 28.9056 - val_loss: 35.7821

Epoch 00060: val_loss did not improve
 - average WER:  0.559207589286
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 61/100
 - 1756s - loss: 28.3496 - val_loss: 35.7653

Epoch 00061: val_loss did not improve
 - average WER:  0.582775297619
Epoch 62/100
 - 1748s - loss: 27.8941 - val_loss: 35.8736

Epoch 00062: val_loss did not improve
 - average WER:  0.568470982143
Epoch 63/100
 - 1754s - loss: 27.3584 - val_loss: 36.6450

Epoch 00063: val_loss did not improve
 - average WER:  0.545107886905
Epoch 64/100
 - 1747s - loss: 26.8155 - val_loss: 36.3513

Epoch 00064: val_loss did not improve
 - average WER:  0.572358630952
Epoch 65/100
 - 1746s - loss: 26.4722 - val_loss: 36.3019

Epoch 00065: val_loss did not improve
 - average WER:  0.561997767857
Epoch 66/100
 - 1746s - loss: 25.9850 - val_loss: 36.3665

Epoch 00066: val_loss did not improve
 - average WER:  0.582570684524
Epoch 67/100
 - 1745s - loss: 25.6145 - val_loss: 36.1138

Epoch 00067: val_loss did not improve
 - average WER:  0.564546130952
Epoch 68/100
 - 1747s - loss: 25.2744 - val_loss: 37.0208

Epoch 00068: val_loss did not improve
 - average WER:  0.561904761905
Epoch 69/100
 - 1746s - loss: 24.6892 - val_loss: 36.3172

Epoch 00069: val_loss did not improve
 - average WER:  0.552771577381
Epoch 70/100
 - 1745s - loss: 24.3922 - val_loss: 36.5702

Epoch 00070: val_loss did not improve
 - average WER:  0.545368303571
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 71/100
 - 1746s - loss: 24.0220 - val_loss: 36.9711

Epoch 00071: val_loss did not improve
 - average WER:  0.507738095238
Epoch 72/100
 - 1745s - loss: 23.6149 - val_loss: 36.7132

Epoch 00072: val_loss did not improve
 - average WER:  0.552380952381
Epoch 73/100
 - 1746s - loss: 23.1794 - val_loss: 37.0830

Epoch 00073: val_loss did not improve
 - average WER:  0.557514880952
Epoch 74/100
 - 1745s - loss: 22.7972 - val_loss: 37.2572

Epoch 00074: val_loss did not improve
 - average WER:  0.583537946429
Epoch 75/100
 - 1743s - loss: 22.5091 - val_loss: 36.6525

Epoch 00075: val_loss did not improve
 - average WER:  0.57150297619
Epoch 76/100
 - 1745s - loss: 22.2403 - val_loss: 37.2367

Epoch 00076: val_loss did not improve
 - average WER:  0.516015625
Epoch 77/100
 - 1744s - loss: 21.9132 - val_loss: 37.9516

Epoch 00077: val_loss did not improve
 - average WER:  0.544177827381
Epoch 78/100
 - 1748s - loss: 21.4916 - val_loss: 38.0761

Epoch 00078: val_loss did not improve
 - average WER:  0.551506696429
Epoch 79/100
 - 1745s - loss: 21.0722 - val_loss: 38.3291

Epoch 00079: val_loss did not improve
 - average WER:  0.542522321429
Epoch 80/100
 - 1743s - loss: 20.8219 - val_loss: 37.2910

Epoch 00080: val_loss did not improve
 - average WER:  0.531789434524
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 81/100
 - 1744s - loss: 20.5189 - val_loss: 37.9017

Epoch 00081: val_loss did not improve
 - average WER:  0.546000744048
Epoch 82/100
 - 1745s - loss: 20.2458 - val_loss: 38.9120

Epoch 00082: val_loss did not improve
 - average WER:  0.565383184524
Epoch 83/100
 - 1750s - loss: 19.7160 - val_loss: 38.1115

Epoch 00083: val_loss did not improve
 - average WER:  0.545870535714
Epoch 84/100
 - 1744s - loss: 19.5226 - val_loss: 39.3296

Epoch 00084: val_loss did not improve
 - average WER:  0.561104910714
Epoch 85/100
 - 1746s - loss: 19.2445 - val_loss: 40.2349

Epoch 00085: val_loss did not improve
 - average WER:  0.563988095238
Epoch 86/100
 - 1744s - loss: 18.9381 - val_loss: 39.4026

Epoch 00086: val_loss did not improve
 - average WER:  0.591443452381
Epoch 87/100
 - 1743s - loss: 18.5678 - val_loss: 38.8290

Epoch 00087: val_loss did not improve
 - average WER:  0.580133928571
Epoch 88/100
 - 1743s - loss: 18.3128 - val_loss: 39.7723

Epoch 00088: val_loss did not improve
 - average WER:  0.559933035714
Epoch 89/100
 - 1749s - loss: 17.9492 - val_loss: 40.5101

Epoch 00089: val_loss did not improve
 - average WER:  0.568247767857
Epoch 90/100
 - 1743s - loss: 17.7327 - val_loss: 40.0685

Epoch 00090: val_loss did not improve
 - average WER:  0.557645089286
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
Epoch 91/100
 - 1743s - loss: 17.4790 - val_loss: 39.9796

Epoch 00091: val_loss did not improve
 - average WER:  0.569289434524
Epoch 92/100
 - 1740s - loss: 17.2659 - val_loss: 39.9866

Epoch 00092: val_loss did not improve
 - average WER:  0.547340029762
Epoch 93/100
 - 1743s - loss: 17.0192 - val_loss: 40.8611

Epoch 00093: val_loss did not improve
 - average WER:  0.569754464286
Epoch 94/100
 - 1743s - loss: 16.7839 - val_loss: 40.4483

Epoch 00094: val_loss did not improve
 - average WER:  0.576636904762
Epoch 95/100
 - 1742s - loss: 16.5012 - val_loss: 40.9576

Epoch 00095: val_loss did not improve
 - average WER:  0.568936011905
Epoch 96/100
 - 1744s - loss: 16.3744 - val_loss: 40.6260

Epoch 00096: val_loss did not improve
 - average WER:  0.532161458333
Epoch 97/100
 - 1740s - loss: 16.1122 - val_loss: 40.7611

Epoch 00097: val_loss did not improve
 - average WER:  0.571912202381
Epoch 98/100
 - 1741s - loss: 15.8740 - val_loss: 42.0989

Epoch 00098: val_loss did not improve
 - average WER:  0.539899553571
Epoch 99/100
 - 1740s - loss: 15.5892 - val_loss: 40.6339

Epoch 00099: val_loss did not improve
 - average WER:  0.557068452381
Epoch 100/100
 - 1742s - loss: 15.3838 - val_loss: 41.9526

Epoch 00100: val_loss did not improve
 - average WER:  0.592317708333
Log file saved:  log_files/1105-5deep-lstm-60_05-11_0850.csv
An exception of type ValueError occurred. Arguments:
('max() arg is an empty sequence',)
Ending time:  2018-05-13 09:37:44
